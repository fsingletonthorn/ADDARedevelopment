---
title: "Laboratory Exercise 10"
subtitle:  "Multilevel modeling 2: Random intercept and random slopes models"
output:
  html_document:
  toc: true
  toc_float: true
  toc_collapsed: true
toc_depth: 3
number_sections: true
theme: lumen
---

```{r setup, include=FALSE, eval=T}
knitr::opts_chunk$set(echo = T,
                      results = "hide",
                     warning = FALSE)

for(package in c('lme4', 'lmerTest', 'tidyverse', 'foreign', 'readr', 'sjstats', 'knitr')) { # start a loop
    if (!require(package, character.only=T, quietly=T)) {
        install.packages(package)
        library(package, character.only=T)
    }
}

```

# {.tabset  .tabset-fade}

## Part 1 - Data importing and tidying 

#### Starting up

As per last week, download and unzip today's data into a new folder and open the file "tutorial_10.Rproj". This will open up R and R studio and make sure that you don't have to worry about file-paths. Make sure that you unzip everything into the same folder.

### Setting up R to run our analyses

Next, try running the following code by copying and pasting (<kbd>Ctrl</kbd>+<kbd>C</kbd>, <kbd>Ctrl</kbd>+<kbd>V</kbd>) it into your R session's console.

```{r eval = F}
# The following code functions ensure that the packages (like sub-programs that R uses to do things) are installed
install.packages(c('lme4', 'lmerTest', 'tidyverse', 'readr', 'sjstats'))
# This bit of code tells R to load and attach (i.e., make avaliable) those packages so that the functions they include can be used
library('lme4')
library('lmerTest')
library('tidyverse')
library('readr')
library('sjstats')
```

#### The data
The data for this week's exercise is the same as last week's - it is an organizational psychology data-set described in Klein et al. (2000), and is stored as *siop_merged.csv*.

#### Variables

The file *siop_merge.csv* contains 750 employee-level observations nested within 50 work-groups. Ignoring the variable group ID (grpid), there are eight standardized variables, where a higher score indicates a higher level (e.g., higher pay or more negative leadership behaviors). These data were collected by an individual survey of the employees, so each variable measures individual level perceptions, except for the variable "physen", which is the group level "physical work environment".

Table 1. The variable names and variables included in *siop_merge.csv*.

```{r, echo=FALSE}
kable(data_frame("Variable name" = c("grpid", "jobsat", "cohes", "posaff", "pay", "neglead", "wload", "tasksig", "physen"), 
           "Description"  = c("Group ID", "Job satisfaction", "Perceived cohesion", "Positive affect", "Pay", "Perceived Negative leader behaviours", "Perceived workload", "Perceived task significance", "physical work enviroment (group level)")))
```



The first thing we need to do is read in the data. Copy and paste the following into your R console to load today's data.

```{r}

siop_merged <- read.csv("siop_merged.csv")

```


<br>
<br>

#### The data 

The first variables we'll be dealing with today are  "Job satisfaction", "Positive affect" and "Physical work environment (group level)". As always, it's a good idea to plot your data. This data has three variables, so you can either plot it with facets, or use a 3D plot. Click and drag this one to have a look at your data:

```{r echo = FALSE, message=FALSE, warning=FALSE}
library(plotly)

  
p <- plot_ly(x = siop_merged$jobsat, z = siop_merged$posaff,
        y = siop_merged$physen) %>% 
  add_trace(type="scatter3d", showlegend = FALSE, opacity = 1,
            color = as.factor(siop_merged$physen))  %>% 
layout(scene = list(xaxis = list(title = 'Job satisfaction'),
                     zaxis = list(title = 'Positive affect'),
                     yaxis = list(title = 'Physical enviroment')))

```

`r p`

Figure 1. A 3D scatterplot of Job satisfaction, positive affect and group physical environment, color indicates group membership.

<br>

#### Question `r q <- 1; q`. 

What is the relationship between positive affect and job satisfaction? Is there a clear directional relationship? 

<br>

## Part 2 - Random intercepts model with a predictor at the group level

Let's fit a random intercept model with one individual level predictor - we'll start with positive affect - and one group level predictor - physical work environment. 

This model has random intercepts for group, a single level 1 predictor (positive affect) and a single group level predictor, and may be written as:

$JobSat_{ij} = \gamma_{00} +  \gamma_{10} PosAff_{ij}+ \gamma_{01}PhysEn_{0j} + u_{0j}  + \epsilon_{ij}$

This means each person's job satisfaction is equal to the overall intercept ($\gamma_{00}$), a regression term times their positive affect ($\gamma_{10}(PosAff_{ij})$), a regression term times the group level predictor ($\gamma_{01}PhysEn_{0j}$), a random effect for their group ($u_{0j}$), plus an error term ($\epsilon_{ij}$) (i.e., the residual, as the MLM will not perfectly predict their score). You can run this model with the following code: 

```{r}
PA_PE_model <- lmer("jobsat ~ 1 + posaff + physen + (1 | grpid)", data = siop_merged)
```

```{r echo = F}

item <-knitr::kable(data_frame("Terms"=c("jobsat ~","1 +", "posaff", "physen", "(1 | grpid)"), "Meaning"= c("job satisfaction is predicted by",
"An intercept parameter which is the same for everyone", 
"a regression coefficient times each person's positive affect", "a regression coefficient times each group's physical enviroment",  "A random intercept for each group")))

```

##### Which should be read as:

`r item`

This model says that the relationship between positive affect and job satisfaction is the same in every group, but that the baseline - the intercept - can vary by group, and that we can predict the group intercepts using the group's score on physical environment. To visualize this, we can plot job satisfaction by positive affect and draw a regression line for each group. The model does not actually specify an intercept for each group, but rather assumes that the intercepts come from a normal distribution with a mean of the overall intercept and a variance as specified in the random effects output above. The output we will often be interested in is the variance of these groups' intercepts as opposed to their individual values. 


Remember *summary(PA_PE_model)* is useful in getting a summary of your output out! 

```{r}

summary(PA_PE_model)

```





<details><summary>Figure 2</summary>
<p>

```{r echo=F}

ggplot(siop_merged, aes(x = posaff, y = jobsat, group = as.factor(grpid), colour = as.factor(grpid))) + 
  geom_point()  + xlab("Positive affect") + ylab("Job satisfaction") + 
  geom_abline(slope = PA_PE_model@beta[2], intercept = coef(PA_PE_model)[[1]][,1]+(coef(PA_PE_model)[[1]][,3]*unique(siop_merged$physen)), colour = as.factor(1:50), alpha = .5) + theme_classic()+ theme(legend.position="none")


```

Figure 2. Job satisfaction by positive affect, lines show the estimated relationship by group, color indicates group membership.

</p>
</details>

<br>



#### Question `r q<-q+1; q`. 

What is the overall relationship between positive affect and job satisfaction? Which value from your model summary tells us about this value?

<br>

#### Question `r q<-q+1; q`. 

$JobSat_{ij} = \gamma_{00} +  \gamma_{10} PosAff_{ij}+ \gamma_{01}PhysEn_{0j} + u_{0j}  + \epsilon_{ij}$

Looking at the equation above and comparing it to your model summary, can you identify which part of the model summary output corresponds to each part of the equation? 

<br>


## Part 3 - Random intercepts and random slopes 

Now we will include a random slope for the relationship between positive affect and job satisfaction as well as a group level predictor for the intercept, but without other predictors for the slope. This means that positive affect becomes both a fixed and a random effect. 

This model with random intercepts and a random slope for group, a single group level predictor (physical work environment), and a single level 1 predictor (positive affect) may be written:

$JobSat_{ij} = \gamma_{00} +  \gamma_{10} PosAff_{ij} + \gamma_{01} PhysEn_{0j} + u_{0j} + u_{1j}PosAff_{ij} + \epsilon_{ij}$


We can break this equation down into two parts, the fixed and random effects. 

The fixed effects say that each observed data point is predicted by an overall intercept ($\gamma_{00}$), a regression term times their positive affect $\gamma_{10} (PosAff_{ij})$, and a group level predictor $\gamma_{01} PhysEn_{0j}$. 

The random effects say that the slopes can change for each group ($u_{1j}PosAff_{ij}$), and the intercepts can change by group  ($u_{0j}$).

```{r}
RS_model <- lmer("jobsat ~ 1 + posaff + physen + (1 + posaff | grpid)", data = siop_merged)
```

```{r echo = F}

item <-knitr::kable(data_frame("Terms"=c("jobsat ~","1 +", "posaff", "physen", "(1 + posaff | grpid)"), "Meaning"= c("job satisfaction is predicted by",
"An intercept parameter which is the same for everyone", 
"a regression coefficient times each person's positive affect", "a regression coefficient times each group's physical enviroment",  "A random intercept and a random slope for the relationship between positive affect and job satisfaction for each group")))

```

##### Which should be read as:

`r item`

This model says that the relationship between positive affect and job satisfaction changes in different groups by some estimated amount, and that we can predict the groups' true mean scores using their group's physical environment.

Remember *summary(RS_model)* is useful in getting a summary of your output out! 


```{r echo = TRUE}
summary(RS_model)
```


To visualize this, we can again plot job satisfaction by positive affect and draw a regression line for each group. The regression line slopes and intercepts change by group, because we now have random slopes and intercepts, allowing the relationship between positive affect and job satisfaction to change by group. 

<details open><summary>Figure 3</summary>
<p>

```{r echo=F}

ggplot(siop_merged, aes(y = jobsat, x = posaff, group = as.factor(grpid), colour = as.factor(grpid))) + 
  geom_point()  + xlab("Positive affect") + ylab("Job satisfaction") + 
  geom_abline(slope = coef(RS_model)[[1]][,2], intercept = coef(RS_model)[[1]][,1]+(coef(RS_model)[[1]][,3]*unique(siop_merged$physen)), colour = as.factor(1:50), alpha = .5) + theme_classic()+ theme(legend.position="none")

```

Figure 3. Job satisfaction by positive affect, lines show the estimated relationship by group, color indicates group.

</p>
</details>

<br>

<br>


#### Question `r q<-q+1; q`.

What are we saying about the relationship between job satisfaction and positive affect by including random slopes? Do you think that this claim is a plausible in the context of the real world?

<br>

## Part 4 - Random intercepts and random slopes with a group level predictor

Let's fit a random intercept and slopes model with one individual level predictor - again positive affect - and one group level predictor - physical work environment, and one group level predictor of the random effects. 

$JobSat_{ij} = \gamma_{00} + \gamma_{10} PosAff_{ij} + \gamma_{01} PhysEn_{0j} + \gamma_{10} (PosAff_{ij} \times PhysEn_{0j}) + u_{0j} + u_{1j}PosAff_{ij} + \epsilon_{ij}$

This means each person's job satisfaction is equal to the overall intercept ($\gamma_{00}$), a regression term times their positive affect ($\gamma_{10} (PosAff_{ij})$), a regression term times the group level physical enviroment ($\gamma_{01} PhysEn_{0j}$), a interaction effect between positive affect and physical work enviroment ($\gamma_{10} (PosAff_{ij} \times PhysEn_{0j})$), a random effect times the individual level predictor giving us random slopes ($u_{1j}PosAff_{ij}$),  a random effect for group ($u_{0j}$), plus an error term for the individual ($\epsilon_{ij}$).

You can run the model in R by copying and pasting the following into your session (make sure you've loaded *siop_merged* first):

```{r}

RS_model_2 <- lmer("jobsat ~ 1 + posaff + physen + posaff * physen + (1 + posaff | grpid)", data = siop_merged)

```

```{r echo = F}

item <-knitr::kable(data_frame("Terms"=c("jobsat ~","1 +", "posaff", "physen", "physen * posaff", "(1 + posaff | grpid)"), "Meaning"= c("job satisfaction is predicted by",
"An intercept parameter which is the same for everyone", 
"a regression coefficient times each person's positive affect", "a regression coefficient times each group's physical enviroment", "a regression coefficient times the product of physical enviroment times positive affect (i.e., an interaction)", "A random intercept and a random slope for the relationship between positive affect and job satisfaction for each group")))

```

##### Which should be read as:

`r item`

```{r}

summary(RS_model_2)

```

This model says that the relationship between positive affect and job satisfaction changes in different groups by some estimated amount, and that we can predict the groups' true mean scores using their group's physical environment, and that the relationship between positive affect and job satisfaction is influenced by the physical environment.

Remember *summary(RS_model_2)* is useful in getting a summary of your output out! 

To visualize this, we can again plot job satisfaction by positive affect and draw a regression line for each group. The regression line slopes and intercepts still change by group because we have random slopes and intercepts, however we are now trying to predict the slope of each group with the variable physical environment.

<details open><summary>Figure 4</summary>
<p>

```{r echo=F}

ggplot(siop_merged, aes(y = jobsat, x = posaff, group = as.factor(grpid), colour = as.factor(grpid))) + 
  geom_point()  + xlab("Positive affect") + ylab("Job satisfaction") + 
  geom_abline(slope = coef(RS_model_2)[[1]][,3] + (coef(RS_model_2)[[1]][,4]*unique(siop_merged$physen)), intercept = coef(RS_model_2)[[1]][,1]+(coef(RS_model_2)[[1]][,2]*unique(siop_merged$physen)), colour = as.factor(1:50), alpha = .5) + theme_classic()+ theme(legend.position="none")


```

Figure 4. Job satisfaction by positive affect, lines show the estimated relationship by group, color indicates group.

</p>
</details>

<br>

#### Question `r q<-q+1; q`.

What are we saying when we include the interaction term in our model? Does it look like it's worth including this term in the model?

<br>

#### Question `r q<-q+1; q`.

How does the variance of the random effect for positive affect change between models?

<br>


#### Question `r q<-q+1; q`.

Looking at each of the models, which do you think is best (take a look at "*summary(PA_PE_model)* and *summary(RS_model)*). Do we even need random slopes? 

Hint: Compare the amount of residual variance in *RS_model* to the amount seen in *PA_PE_model*. Does the increased complexity of the random slopes model seem worth the reduction in residual variance? 

<br>



### If you have any time left, have a play around with different predictor and predicted variables!   


<br>

## Model output

### Random intercepts
```{r echo=TRUE, results = TRUE}
summary(PA_PE_model)
```

<br>

### Random intercepts and slopes 

```{r echo=TRUE, results = TRUE}
summary(RS_model)
```

<br>

### Random intercepts and slopes with a group level predictor

```{r echo=TRUE, results = TRUE}
summary(RS_model_2)
```


## Question Answers

#### Question 1. 

*What is the relationship between positive affect and job satisfaction? Is there a clear directional relationship?*

Taking a look at figure 1, it looks like positive affect is positively correlated with job satisfaction.

<br>

<br>

### Questions about the random intercepts model (PA_PE_model)

#### Question 2.

*What is the overall relationship between positive affect and job satisfaction? Which value from your model summary tells us about this value?*

This model suggests that there is a positive relationship between positive affect and job satisfaction, with an increase of one unit of positive affect leading to a predicted increase of 0.428 units of job satisfaction. 

<br>

<br>


#### Question 3.

$JobSat_{ij} = \gamma_{00} +  \gamma_{10} PosAff_{ij}+ \gamma_{01}PhysEn_{0j} + u_{0j}  + \epsilon_{ij}$

*Looking at the equation above and comparing it to your model summary, can you identify which part of the model summary output corresponds to each part of the equation?*

<br>

![](images/question3outputHighlight.PNG)

<br>

<br>


### Questions about the random intercepts and slopes models

#### Question 4.

*What are we saying about the relationship between job satisfaction and positive affect by including random slopes? Do you think that this claim is a plausible in the context of the real world?*

We are saying that the relationship between positive affect and job satisfaction may differ by work group. In the real world, there are lots of scenarios in which this might be a reasonable assumption - the relationship between job satisfaction and positive affect is likely different in, say, a toy shop compared to a meat packing plant. 

<br>

<br>

#### Question 5.

*What are we saying when we include the interaction term in our model?* 

We are saying that the relationship between job satisfaction and positive affect can be predicted using the physical work environment.
<br>

<br>

#### Question 6. 
*How does the variance of the random effect for positive affect change between models?*

The residual variance actually *increases* - without interaction: 0.04955, going to  0.05875 with the interaction. This is something that can never happen when adding terms in normal multiple regression, but can under rare circumstances in multilevel models. 

You do **not** need to know why, but this paper http://www.stat.columbia.edu/~gelman/stuff_for_blog/adding.pdf gives as readable an explanation as is available! In the context of interactions it is very difficult to figure out what this result implies and I recommend spending precious thinking time elsewhere.

<br>

<br>

#### Question 7.
*Looking at each of the models, which do you think is best (take a look at summary(PA_PE_model) and summary(RS_model)). Do we even need random slopes?*

*Hint: Compare the amount of residual variance in RS_model to the amount seen in PA_PE_model. Does the increased complexity of the random slopes model seem worth the reduction in residual variance?*

There is a very small reduction in residual variance when including the random slopes, going from 5.3205 to 5.27734, a reduction of less than 1% (i.e., $((5.3205 - 5.27734)/5.3205)*100$). However, this term may make enough sense on purely conceptual grounds that excluding it from the model would be ill advised. Without an in depth understanding of the theoretical context, it's difficult to make concrete suggestions.

<br>

<br>


